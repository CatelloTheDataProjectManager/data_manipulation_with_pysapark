{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "507363b7-1f39-48c3-a7f7-5076d59e81f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ff36abda06ff:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7ff686255010>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b34e63a-ecb3-4fee-9c9a-736a76e65228",
   "metadata": {},
   "source": [
    "### 1.Broadcast variable\n",
    "\n",
    "In PySpark, a broadcast variable is a read-only variable that is cached on each node in a cluster. Broadcast variables are used to efficiently distribute large, read-only datasets to all nodes in a cluster for use in parallel processing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc688a4d-f9dd-41ba-8148-88edcfbfcc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "status_map = {\"M\" : \"Married\", \"S\": \"Single\"}\n",
    "\n",
    "data = [\n",
    "    (1,\"Kim\",\"M\", 74),\n",
    "    (2,\"Lee\",\"S\", 30),\n",
    "    (3,\"Choi\",\"M\", 45),\n",
    "    (4,\"Park\",\"S\", 28),\n",
    "    (5,\"Han\",\"M\", 55),\n",
    "    (6,\"Cho\",\"S\", 32)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3908cf25-9786-45b6-b4da-c387451cff13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      "\n",
      "+---+----+------+---+\n",
      "| id|name|status|age|\n",
      "+---+----+------+---+\n",
      "|  1| Kim|     M| 74|\n",
      "|  2| Lee|     S| 30|\n",
      "|  3|Choi|     M| 45|\n",
      "|  4|Park|     S| 28|\n",
      "|  5| Han|     M| 55|\n",
      "|  6| Cho|     S| 32|\n",
      "+---+----+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# creating broadcast variable and storing the information\n",
    "broadcast_status = sc.broadcast(status_map)\n",
    "\n",
    "df = spark.createDataFrame(data, schema=[\"id\", \"name\", \"status\", \"age\"])\n",
    "df.printSchema()\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49375749-f1c0-4c3f-9683-b3f1ccc45d64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Married'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broadcast_status.value[\"M\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89725a8e-6f1e-4013-b7fe-91a29d36036b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+---+-----------+\n",
      "| id|name|status|age|status_info|\n",
      "+---+----+------+---+-----------+\n",
      "|  1| Kim|     M| 74|    Married|\n",
      "|  2| Lee|     S| 30|     Single|\n",
      "|  3|Choi|     M| 45|    Married|\n",
      "|  4|Park|     S| 28|     Single|\n",
      "|  5| Han|     M| 55|    Married|\n",
      "|  6| Cho|     S| 32|     Single|\n",
      "+---+----+------+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, udf\n",
    "\n",
    "def convert_status(char):\n",
    "    return broadcast_status.value[char]\n",
    "\n",
    "convert_status_udf = udf(convert_status)\n",
    "\n",
    "df_2 = df.withColumn(\"status_info\", convert_status_udf(col(\"status\")))\n",
    "df_2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0a9a31-4581-489d-b7f5-c3b829af1fde",
   "metadata": {},
   "source": [
    "### 2.Partition\n",
    "\n",
    "In PySpark, a partition is a logical division of data that is used to distribute data across nodes in a cluster for parallel processing. Partitioning is a key concept in PySpark and is used to optimize the performance of data processing tasks.\n",
    "\n",
    "- **Data Shuffle**  refers to the process of redistributing data across nodes in a cluster during parallel processing tasks. Shuffling is typically required when performing operations like joins, aggregations, and sorting, which require data to be reorganized based on specific keys\n",
    "\n",
    "- **Data skew** refers to the phenomenon where certain keys in a dataset have a disproportionately large number of values associated with them. When data is skewed, some nodes in the cluster may receive significantly more data than others, leading to an imbalance in processing workload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11362e70-b771-42e4-b249-7d36f54d65db",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "(\"Alice\", 35, 'HR', 3000),\n",
    "(\"Charlie\", 40, 'Sales', 4000),\n",
    "(\"David\", 28, 'Marketing', 2500),\n",
    "(\"Eve\", 32, 'Finance', 5000),\n",
    "(\"Frank\", 45, 'IT', 6000),\n",
    "(\"Grace\", 29, 'HR', 3500),\n",
    "(\"Heidi\", 38, 'Sales', 4500),\n",
    "(\"Ivan\", 49, 'Marketing', 5500),\n",
    "(\"Judy\", 31, 'Finance', 6500),\n",
    "(\"Karen\", 50, 'IT', 7000),\n",
    "(\"Leo\", 37, 'HR', 3700),\n",
    "(\"Mike\", 44, 'Sales', 4400)\n",
    "]\n",
    "\n",
    "col = [\"Name\", \"Age\", \"Department\", \"Salary\"]\n",
    "\n",
    "employeedf = spark.createDataFrame(data = dataset, schema = col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47a4e3e7-ac3f-4252-85d4-bd251e3cc8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+\n",
      "|   Name|Age|Department|Salary|\n",
      "+-------+---+----------+------+\n",
      "|  Alice| 35|        HR|  3000|\n",
      "|Charlie| 40|     Sales|  4000|\n",
      "|  David| 28| Marketing|  2500|\n",
      "|    Eve| 32|   Finance|  5000|\n",
      "|  Frank| 45|        IT|  6000|\n",
      "|  Grace| 29|        HR|  3500|\n",
      "|  Heidi| 38|     Sales|  4500|\n",
      "|   Ivan| 49| Marketing|  5500|\n",
      "|   Judy| 31|   Finance|  6500|\n",
      "|  Karen| 50|        IT|  7000|\n",
      "|    Leo| 37|        HR|  3700|\n",
      "|   Mike| 44|     Sales|  4400|\n",
      "+-------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeedf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e89547-0d20-4f7a-b926-9f23cd1c8a7f",
   "metadata": {},
   "source": [
    "RDD stands for **Resilient Distributed Datasets**. It is a fundamental data structure in Apache Spark that represents an immutable distributed collection of objects, which can be processed in parallel across a cluster of machines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30810598-b8d1-40c0-9977-aaa4d8a0373d",
   "metadata": {},
   "source": [
    "##### partitionBy\n",
    "It partitions the data based on the specified column and writes each partition to a separate file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b42306ec-ea2e-4116-b195-d137633b4df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "employeedf.write.option(\"header\",True) \\\n",
    "        .partitionBy(\"Department\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .csv(\"employee\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17a2d55-7df8-472a-ad59-8c1b8dd342d7",
   "metadata": {},
   "source": [
    "##### coalesce\n",
    "It reduces the number of partitions in an RDD without shuffling data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f8d138a-61f0-46e2-9856-b771aa755fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "employeedf.coalesce(2) \\\n",
    "        .write.option(\"header\",True) \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .csv(\"emp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847d24aa-6c97-4313-845a-7c9dc14b54f6",
   "metadata": {},
   "source": [
    "##### repartition\n",
    "This can be useful when the existing partitions are not balanced, or when you want to increase or decrease the number of partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c5813d9e-e71f-4910-8fef-d52d5a6b09c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "employeedf.repartition(2) \\\n",
    "        .write.option(\"header\",True) \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .csv(\"empl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef59a59f-48cd-45d0-ad0c-1c3c35eca55c",
   "metadata": {},
   "source": [
    "### 3.withColumn function\n",
    "\n",
    "The withColumn() function in PySpark is used to add a new column to an existing DataFrame or replace an existing column with a new one. The function takes two arguments: the name of the new column and the column expression that defines the values for the new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "10add904-4d8f-4a08-baa5-95a4e290e9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+\n",
      "|   Name|Age|Department|Salary|\n",
      "+-------+---+----------+------+\n",
      "|  Alice| 35|        HR|  3000|\n",
      "|Charlie| 40|     Sales|  4000|\n",
      "|  David| 28| Marketing|  2500|\n",
      "|    Eve| 32|   Finance|  5000|\n",
      "|  Frank| 45|        IT|  6000|\n",
      "|  Grace| 29|        HR|  3500|\n",
      "|  Heidi| 38|     Sales|  4500|\n",
      "|   Ivan| 49| Marketing|  5500|\n",
      "|   Judy| 31|   Finance|  6500|\n",
      "|  Karen| 50|        IT|  7000|\n",
      "|    Leo| 37|        HR|  3700|\n",
      "|   Mike| 44|     Sales|  4400|\n",
      "+-------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeedf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f072f1-d46f-41a6-98e7-0cac87a2544c",
   "metadata": {},
   "source": [
    "##### Update existing column using values of another existing column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "494309e7-da03-492d-9985-9f9d459a6d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+-------------------+--------------------+\n",
      "|   Name|Age|Department|Salary|percentage_increase|        current_time|\n",
      "+-------+---+----------+------+-------------------+--------------------+\n",
      "|  Alice| 35|        HR|  3000|                 10|2024-03-22 14:25:...|\n",
      "|Charlie| 40|     Sales|  4000|                 10|2024-03-22 14:25:...|\n",
      "|  David| 28| Marketing|  2500|                 10|2024-03-22 14:25:...|\n",
      "|    Eve| 32|   Finance|  5000|                 10|2024-03-22 14:25:...|\n",
      "|  Frank| 45|        IT|  6000|                 10|2024-03-22 14:25:...|\n",
      "|  Grace| 29|        HR|  3500|                 10|2024-03-22 14:25:...|\n",
      "|  Heidi| 38|     Sales|  4500|                 10|2024-03-22 14:25:...|\n",
      "|   Ivan| 49| Marketing|  5500|                 10|2024-03-22 14:25:...|\n",
      "|   Judy| 31|   Finance|  6500|                 10|2024-03-22 14:25:...|\n",
      "|  Karen| 50|        IT|  7000|                 10|2024-03-22 14:25:...|\n",
      "|    Leo| 37|        HR|  3700|                 10|2024-03-22 14:25:...|\n",
      "|   Mike| 44|     Sales|  4400|                 10|2024-03-22 14:25:...|\n",
      "+-------+---+----------+------+-------------------+--------------------+\n",
      "\n",
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: long (nullable = true)\n",
      " |-- Department: string (nullable = true)\n",
      " |-- Salary: long (nullable = true)\n",
      " |-- percentage_increase: integer (nullable = false)\n",
      " |-- current_time: timestamp (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit, current_timestamp\n",
    "\n",
    "employeedf = employeedf.withColumn(\"percentage_increase\", lit(10)) \\\n",
    "                       .withColumn(\"current_time\", current_timestamp())\n",
    "employeedf.show()\n",
    "employeedf.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52a183a-b985-4c64-a950-c0f9d2fea207",
   "metadata": {},
   "source": [
    "##### Update existing column based on another column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2e514785-b568-4280-9bb0-5b06399dc96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+-------------------+--------------------+\n",
      "|   Name|Age|Department|Salary|percentage_increase|        current_time|\n",
      "+-------+---+----------+------+-------------------+--------------------+\n",
      "|  Alice| 35|        HR|3300.0|                 10|2024-03-22 14:28:...|\n",
      "|Charlie| 40|     Sales|4400.0|                 10|2024-03-22 14:28:...|\n",
      "|  David| 28| Marketing|2750.0|                 10|2024-03-22 14:28:...|\n",
      "|    Eve| 32|   Finance|5500.0|                 10|2024-03-22 14:28:...|\n",
      "|  Frank| 45|        IT|6600.0|                 10|2024-03-22 14:28:...|\n",
      "|  Grace| 29|        HR|3850.0|                 10|2024-03-22 14:28:...|\n",
      "|  Heidi| 38|     Sales|4950.0|                 10|2024-03-22 14:28:...|\n",
      "|   Ivan| 49| Marketing|6050.0|                 10|2024-03-22 14:28:...|\n",
      "|   Judy| 31|   Finance|7150.0|                 10|2024-03-22 14:28:...|\n",
      "|  Karen| 50|        IT|7700.0|                 10|2024-03-22 14:28:...|\n",
      "|    Leo| 37|        HR|4070.0|                 10|2024-03-22 14:28:...|\n",
      "|   Mike| 44|     Sales|4840.0|                 10|2024-03-22 14:28:...|\n",
      "+-------+---+----------+------+-------------------+--------------------+\n",
      "\n",
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: long (nullable = true)\n",
      " |-- Department: string (nullable = true)\n",
      " |-- Salary: double (nullable = true)\n",
      " |-- percentage_increase: integer (nullable = false)\n",
      " |-- current_time: timestamp (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "employeedf = employeedf.withColumn(\"Salary\", col(\"Salary\") + (col(\"Salary\") * (col(\"percentage_increase\") * 0.01)))\n",
    "employeedf.show()\n",
    "employeedf.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832b8bd8-8f94-4b36-ad81-a67fa70bf3ba",
   "metadata": {},
   "source": [
    "##### Changing datatype a column using withColumn transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cfa8efee-8805-4d13-aa96-2f962b5a830e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+-------------------+--------------------+\n",
      "|   Name|Age|Department|salary|percentage_increase|        current_time|\n",
      "+-------+---+----------+------+-------------------+--------------------+\n",
      "|  Alice| 35|        HR|  3300|                 10|2024-03-22 14:34:...|\n",
      "|Charlie| 40|     Sales|  4400|                 10|2024-03-22 14:34:...|\n",
      "|  David| 28| Marketing|  2750|                 10|2024-03-22 14:34:...|\n",
      "|    Eve| 32|   Finance|  5500|                 10|2024-03-22 14:34:...|\n",
      "|  Frank| 45|        IT|  6600|                 10|2024-03-22 14:34:...|\n",
      "|  Grace| 29|        HR|  3850|                 10|2024-03-22 14:34:...|\n",
      "|  Heidi| 38|     Sales|  4950|                 10|2024-03-22 14:34:...|\n",
      "|   Ivan| 49| Marketing|  6050|                 10|2024-03-22 14:34:...|\n",
      "|   Judy| 31|   Finance|  7150|                 10|2024-03-22 14:34:...|\n",
      "|  Karen| 50|        IT|  7700|                 10|2024-03-22 14:34:...|\n",
      "|    Leo| 37|        HR|  4070|                 10|2024-03-22 14:34:...|\n",
      "|   Mike| 44|     Sales|  4840|                 10|2024-03-22 14:34:...|\n",
      "+-------+---+----------+------+-------------------+--------------------+\n",
      "\n",
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: long (nullable = true)\n",
      " |-- Department: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- percentage_increase: integer (nullable = false)\n",
      " |-- current_time: timestamp (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeedf = employeedf.withColumn(\"salary\",col(\"salary\").cast(\"Integer\"))\n",
    "employeedf.show()\n",
    "employeedf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137fa0a1-93a8-4235-8945-305a7f0a8598",
   "metadata": {},
   "source": [
    "##### Conditional update using withColumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "945b4f7f-47ee-4319-b794-239c647be269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+--------------------+------+-------------------+--------------------+\n",
      "|   Name|Age|          Department|salary|percentage_increase|        current_time|\n",
      "+-------+---+--------------------+------+-------------------+--------------------+\n",
      "|  Alice| 35|                  HR|  3300|                 10|2024-03-22 14:41:...|\n",
      "|Charlie| 40|               Sales|  4400|                 10|2024-03-22 14:41:...|\n",
      "|  David| 28|           Marketing|  2750|                 10|2024-03-22 14:41:...|\n",
      "|    Eve| 32|             Finance|  5500|                 10|2024-03-22 14:41:...|\n",
      "|  Frank| 45|Information Techn...|  6600|                 10|2024-03-22 14:41:...|\n",
      "|  Grace| 29|                  HR|  3850|                 10|2024-03-22 14:41:...|\n",
      "|  Heidi| 38|               Sales|  4950|                 10|2024-03-22 14:41:...|\n",
      "|   Ivan| 49|           Marketing|  6050|                 10|2024-03-22 14:41:...|\n",
      "|   Judy| 31|             Finance|  7150|                 10|2024-03-22 14:41:...|\n",
      "|  Karen| 50|Information Techn...|  7700|                 10|2024-03-22 14:41:...|\n",
      "|    Leo| 37|                  HR|  4070|                 10|2024-03-22 14:41:...|\n",
      "|   Mike| 44|               Sales|  4840|                 10|2024-03-22 14:41:...|\n",
      "+-------+---+--------------------+------+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "employeedf = employeedf.withColumn('Department', when(employeedf['Department'] == \"IT\",\n",
    "                 \"Information Technology\").otherwise(employeedf['Department']))\n",
    "employeedf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4497ec1-ccaa-4001-9681-8f3469e6239e",
   "metadata": {},
   "source": [
    "##### Renaming a column name using withColumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f26fa052-3b7d-4ad8-9d30-7f6b019f2033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+--------------------+------+----------------+--------------------+\n",
      "|   Name|Age|          Department|salary|bonus_percentage|        current_time|\n",
      "+-------+---+--------------------+------+----------------+--------------------+\n",
      "|  Alice| 35|                  HR|  3300|              10|2024-03-22 14:43:...|\n",
      "|Charlie| 40|               Sales|  4400|              10|2024-03-22 14:43:...|\n",
      "|  David| 28|           Marketing|  2750|              10|2024-03-22 14:43:...|\n",
      "|    Eve| 32|             Finance|  5500|              10|2024-03-22 14:43:...|\n",
      "|  Frank| 45|Information Techn...|  6600|              10|2024-03-22 14:43:...|\n",
      "|  Grace| 29|                  HR|  3850|              10|2024-03-22 14:43:...|\n",
      "|  Heidi| 38|               Sales|  4950|              10|2024-03-22 14:43:...|\n",
      "|   Ivan| 49|           Marketing|  6050|              10|2024-03-22 14:43:...|\n",
      "|   Judy| 31|             Finance|  7150|              10|2024-03-22 14:43:...|\n",
      "|  Karen| 50|Information Techn...|  7700|              10|2024-03-22 14:43:...|\n",
      "|    Leo| 37|                  HR|  4070|              10|2024-03-22 14:43:...|\n",
      "|   Mike| 44|               Sales|  4840|              10|2024-03-22 14:43:...|\n",
      "+-------+---+--------------------+------+----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeedf = employeedf.withColumnRenamed(\"percentage_increase\",\"bonus_percentage\") \n",
    "\n",
    "employeedf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f208226-8771-4fe3-985b-2d1e194cc34d",
   "metadata": {},
   "source": [
    "### 4.Pivot & Unpivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d78f7ca3-e733-4802-bf00-ce6efadeb57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Country: string (nullable = true)\n",
      " |-- medal: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n",
      "+-------------+------+-----+\n",
      "|      Country| medal|count|\n",
      "+-------------+------+-----+\n",
      "|United States|  Gold|   39|\n",
      "|United States|Silver|   41|\n",
      "|United States|Bronze|   33|\n",
      "|        China|  Gold|   38|\n",
      "|        China|Silver|   32|\n",
      "|        China|Bronze|   19|\n",
      "|        Japan|  Gold|   27|\n",
      "|        Japan|Silver|   14|\n",
      "|        Japan|Bronze|   17|\n",
      "|Great Britain|  Gold|   22|\n",
      "|Great Britain|Silver|   20|\n",
      "|Great Britain|Bronze|   22|\n",
      "+-------------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "olympics = [(\"United States\", \"Gold\", 39), \n",
    "             (\"United States\", \"Silver\", 41),\n",
    "             (\"United States\", \"Bronze\", 33),\n",
    "             (\"China\", \"Gold\", 38), \n",
    "             (\"China\", \"Silver\", 32),\n",
    "             (\"China\", \"Bronze\", 19),\n",
    "             (\"Japan\", \"Gold\", 27), \n",
    "             (\"Japan\", \"Silver\", 14),\n",
    "             (\"Japan\", \"Bronze\", 17),\n",
    "             (\"Great Britain\", \"Gold\", 22), \n",
    "             (\"Great Britain\", \"Silver\", 20),\n",
    "             (\"Great Britain\", \"Bronze\", 22)          \n",
    "             ]\n",
    "\n",
    "\n",
    "col = ['Country', 'medal', 'count']\n",
    "\n",
    "olympicsdf = spark.createDataFrame(data = olympics,schema = col)\n",
    "\n",
    "olympicsdf.printSchema()\n",
    "olympicsdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808f9c79-fcb0-4529-8efc-29e7ab02154e",
   "metadata": {},
   "source": [
    "##### groupBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "49738735-5b71-44a5-850f-dc8767b6b2e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Country: string (nullable = true)\n",
      " |-- sum(count): long (nullable = true)\n",
      "\n",
      "+-------------+----------+\n",
      "|Country      |sum(count)|\n",
      "+-------------+----------+\n",
      "|United States|113       |\n",
      "|China        |89        |\n",
      "|Japan        |58        |\n",
      "|Great Britain|64        |\n",
      "+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivotDF = olympicsdf.groupBy(\"Country\").sum(\"count\")\n",
    "pivotDF.printSchema()\n",
    "pivotDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb36e31-0d66-4077-84de-473ff34536c0",
   "metadata": {},
   "source": [
    "##### groupBy + pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2f65e887-34d2-4f9e-a0a7-3e539aebd75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Bronze: long (nullable = true)\n",
      " |-- Gold: long (nullable = true)\n",
      " |-- Silver: long (nullable = true)\n",
      "\n",
      "+-------------+------+----+------+\n",
      "|Country      |Bronze|Gold|Silver|\n",
      "+-------------+------+----+------+\n",
      "|Great Britain|22    |22  |20    |\n",
      "|United States|33    |39  |41    |\n",
      "|China        |19    |38  |32    |\n",
      "|Japan        |17    |27  |14    |\n",
      "+-------------+------+----+------+\n",
      "\n",
      "+-------------+----+------+\n",
      "|Country      |Gold|Silver|\n",
      "+-------------+----+------+\n",
      "|Great Britain|22  |20    |\n",
      "|United States|39  |41    |\n",
      "|China        |38  |32    |\n",
      "|Japan        |27  |14    |\n",
      "+-------------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivotDF = olympicsdf.groupBy(\"Country\").pivot(\"medal\").sum(\"count\")\n",
    "pivotDF.printSchema()\n",
    "pivotDF.show(truncate=False)\n",
    "\n",
    "\n",
    "pivotDF1 = olympicsdf.groupBy(\"Country\").pivot(\"medal\",['Gold','Silver']).sum(\"count\")\n",
    "pivotDF1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af88b801-abda-4a7a-ac96-f705d0089543",
   "metadata": {},
   "source": [
    "##### unpivot + stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "04e3ed07-6470-46d2-baf7-7ab6d8e3f2e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+-----+\n",
      "|      Country|medal|Total|\n",
      "+-------------+-----+-----+\n",
      "|Great Britain|    G|   22|\n",
      "|Great Britain|    S|   20|\n",
      "|Great Britain|    B|   22|\n",
      "|United States|    G|   39|\n",
      "|United States|    S|   41|\n",
      "|United States|    B|   33|\n",
      "|        China|    G|   38|\n",
      "|        China|    S|   32|\n",
      "|        China|    B|   19|\n",
      "|        Japan|    G|   27|\n",
      "|        Japan|    S|   14|\n",
      "|        Japan|    B|   17|\n",
      "+-------------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "unpivotExpr = \"stack(3, 'G', Gold, 'S', Silver, 'B', Bronze) as (medal,Total)\"\n",
    "unPivotDF = pivotDF.select(\"Country\", expr(unpivotExpr)) \n",
    "unPivotDF.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
